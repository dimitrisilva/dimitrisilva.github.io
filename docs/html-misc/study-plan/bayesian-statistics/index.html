<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="bayesian-statistics">
  <meta name="robots" content="noindex, nofollow">
  <title>Bayesian Statistics</title>
  <style>
    @font-face {
      font-family: 'Latin Modern Roman';
      src: format('woff2'), format('woff');
      font-weight: normal;
      font-style: normal;
    }
    body {
      background-color: rgb(196, 196, 196);
      padding: 10px 20% 10px 20%;
      text-align: justify;
      font-family: 'Latin Modern Roman', serif;
      line-height: 1.6;
    }
    h1, h2 {
      text-align: center;
    }
    .module-unread {
      color: rgb(0, 0, 0);
    }
    .module-read {
      color: rgb(128, 128, 128);
    }
    .module-item {
      font-weight: bold;
    }
    details {
      margin-bottom: 20px;
      border: 1px solid #ccc;
      border-radius: 5px;
      padding: 10px;
    }
    summary {
      font-weight: bold;
      cursor: pointer;
    }
    ul {
      list-style-type: disc;
      margin-left: 20px;
    }
  </style>
</head>
<body>
  <h1>Bayesian Statistics</h1>
  <p>This self-study plan offers a comprehensive, structured approach to mastering Bayesian data analysis, tailored to those interested in demography and political science. Centered on the theoretical depth of Gelman et al. (2014), it integrates motivating demographic applications from Bryant and Zhang (2018) and hands-on Python implementations from Martin (2024) using tools like PyMC, ArviZ, and Bambi. The plan is divided into two sequential parts: Part 1 (12 weeks) builds foundational to intermediate skills across Gelman Chapters 1-13, while Part 2 (6 weeks) explores advanced topics in Chapters 18-23. Assuming 5-10 hours per week, it spans approximately 18 weeks, with each module including prerequisites, a learning overview, and focused readings/activities. Progress by marking modules as "read" to visually track completion.</p>
  <br>
  
  <h2>Part 1: Bayesian Fundamentals to Advanced Computation (Weeks 1-12)</h2>
  
  <!-- Module 01 -->
  <details class="module-read">
    <summary class="module-summary">Module 1: Basics of Probability and Single/Multi-Parameter Models (Weeks 1-2)</summary>
    <div id="mod1">
      <p>In this module, you'll learn the core principles of Bayesian inference, including how to update beliefs with data using probabilities, construct simple models for single parameters (like rates or means), and extend to multiple parameters while understanding posterior distributions and conjugate priors. This builds a foundational mindset for treating parameters as random variables and interpreting results probabilistically.</p>
      <p><span id="mod1-prereq" class="module-item">Prerequisites</span>. None (assumes basic probability and statistics knowledge; you've started Gelman early chapters).</p>
      <p><span id="mod1-theory" class="module-item">Theory</span>. Gelman et al. (2014) Chapters 1-3 ("Probability and Inference," "Single-Parameter Models," "Introduction to Multiparameter Models"). Focus on concepts like conditional probability, Bayes' theorem, and posterior summarization.</p>
      <p><span id="mod1-apps" class="module-item">Applications</span>. Bryant and Zhang (2018) Chapter 11 ("Mortality in Sweden") for basic rate estimation in demographic contexts, such as modeling infant mortality rates; supplement with Gelman's political science example of election polling in Chapter 2.</p>
      <p><span id="mod1-hands" class="module-item">Hands-on</span>. Martin (2024) Chapters 1-2 ("Thinking Probabilistically," "Programming Probabilistically"). Implement simple models like coin flips or Gaussian inferences in PyMC, practice posterior summarization with ArviZ, and explore posterior predictive checks.</p>
    </div>
  </details>
  
  <!-- Module 02 -->
  <details class="module-read">
    <summary class="module-summary">Module 2: Connections and Hierarchies (Weeks 3-4)</summary>
    <div id="mod2">
      <p>Here, you'll explore how Bayesian methods relate to classical statistics (e.g., asymptotics, maximum likelihood) and introduce hierarchical models to handle grouped data, emphasizing exchangeability and partial pooling for improved estimation in structured datasets.</p>
      <p><span id="mod2-prereq" class="module-item">Prerequisites</span>. Basic probability, single/multi-parameter models, and posterior inference from Module 1.</p>
      <p><span id="mod2-theory" class="module-item">Theory</span>. Gelman et al. (2014) Chapters 4-5 ("Asymptotics and Connections to Non-Bayesian Approaches," "Hierarchical Models"). Emphasize links to frequentist methods and hierarchical setups for varying parameters.</p>
      <p><span id="mod2-apps" class="module-item">Applications</span>. Bryant and Zhang (2018) Chapter 12 ("Life Expectancy in Portugal") for hierarchical modeling of rates across ages and time; include Gelman's example of hierarchical polling data in Chapter 5 for political science motivation.</p>
      <p><span id="mod2-hands" class="module-item">Hands-on</span>. Martin (2024) Chapter 3 ("Hierarchical Models"). Code hierarchical shifts and shrinkage examples in PyMC, such as water quality data, to practice pooling and group comparisons.</p>
    </div>
  </details>
  
  <!-- Module 03 -->
  <details class="module-read">
    <summary class="module-summary">Module 3: Model Evaluation and Data Collection (Weeks 5-6)</summary>
    <div id="mod3">
      <p>This module focuses on assessing model fit through checking and comparison, expanding models for better accuracy, and accounting for data collection processes like surveys or experiments to ensure realistic inferences.</p>
      <p><span id="mod3-prereq" class="module-item">Prerequisites</span>. Hierarchical models and asymptotic connections from Module 2.</p>
      <p><span id="mod3-theory" class="module-item">Theory</span>. Gelman et al. (2014) Chapters 6-8 ("Model Checking," "Evaluating, Comparing, and Expanding Models," "Modeling Accounting for Data Collection"). Cover predictive checks, information criteria, and survey sampling.</p>
      <p><span id="mod3-apps" class="module-item">Applications</span>. Bryant and Zhang (2018) Chapter 13 ("Health Expenditure in the Netherlands") for evaluating predictive accuracy in demographic accounts; pair with Gelman's political survey examples in Chapter 8 (e.g., stratified sampling in elections).</p>
      <p><span id="mod3-hands" class="module-item">Hands-on</span>. Martin (2024) Chapter 5 ("Comparing Models"). Use ArviZ for posterior predictive checks, information criteria like WAIC, and model averaging exercises.</p>
    </div>
  </details>
  
  <!-- Module 04 -->
  <details class="module-unread">
    <summary class="module-summary">Module 4: Decision Analysis and Intro Computation (Weeks 7-8)</summary>
    <div id="mod4">
      <p>You'll delve into Bayesian decision theory for practical choices under uncertainty and introduce computational methods like simulation and integration to approximate posteriors in complex models.</p>
      <p><span id="mod4-prereq" class="module-item">Prerequisites</span>. Model evaluation and data collection strategies from Module 3.</p>
      <p><span id="mod4-theory" class="module-item">Theory</span>. Gelman et al. (2014) Chapters 9-10 ("Decision Analysis," "Introduction to Bayesian Computation"). Discuss utility functions, expected losses, and basic numerical methods.</p>
      <p><span id="mod4-apps" class="module-item">Applications</span>. Bryant and Zhang (2018) Chapter 9 ("Inference about Derived Quantities") for decision-making in forecasting basics, such as population projections; add Gelman's decision examples in political contexts like policy evaluation.</p>
      <p><span id="mod4-hands" class="module-item">Hands-on</span>. Martin (2024) Chapter 10 ("Inference Engines"). Practice grid/quadratic approximations and basic MCMC in PyMC for decision-related simulations.</p>
    </div>
  </details>
  
  <!-- Module 05 -->
  <details class="module-unread">
    <summary class="module-summary">Module 5: Markov Chains (Weeks 9-10)</summary>
    <div id="mod5">
      <p>This module covers Markov chain Monte Carlo (MCMC) techniques for sampling from posteriors, including Gibbs, Metropolis, and Hamiltonian methods, with emphasis on efficiency and diagnostics for reliable inference.</p>
      <p><span id="mod5-prereq" class="module-item">Prerequisites</span>. Basic computation and decision analysis from Module 4.</p>
      <p><span id="mod5-theory" class="module-item">Theory</span>. Gelman et al. (2014) Chapters 11-12 ("Basics of Markov Chain Simulation," "Computationally Efficient Markov Chain Simulation"). Focus on chain convergence, tuning, and advanced samplers.</p>
      <p><span id="mod5-apps" class="module-item">Applications</span>. Bryant and Zhang (2018) Chapter 15 ("Migration in Iceland") for MCMC in handling unreliable or sparse demographic data; incorporate Gelman's MCMC applications to political time series.</p>
      <p><span id="mod5-hands" class="module-item">Hands-on</span>. Martin (2024) Chapter 10 ("Inference Engines"). Implement Metropolis-Hastings and Hamiltonian Monte Carlo in PyMC, with diagnostics like trace plots and R-hat.</p>
    </div>
  </details>
  
  <!-- Module 06 -->
  <details class="module-unread">
    <summary class="module-summary">Module 6: Approximations (Weeks 11-12)</summary>
    <div id="mod6">
      <p>You'll learn variational and other approximation techniques (e.g., EM algorithms, Laplace) as faster alternatives to full MCMC for large-scale problems, understanding trade-offs in accuracy and speed.</p>
      <p><span id="mod6-prereq" class="module-item">Prerequisites</span>. MCMC methods from Module 5.</p>
      <p><span id="mod6-theory" class="module-item">Theory</span>. Gelman et al. (2014) Chapter 13 ("Modal and Distributional Approximations"). Explore normal approximations, variational inference, and expectation-maximization.</p>
      <p><span id="mod6-apps" class="module-item">Applications</span>. Bryant and Zhang (2018) Chapter 16 ("Fertility in Cambodia") for approximations in complex demographic inference with multiple components.</p>
      <p><span id="mod6-hands" class="module-item">Hands-on</span>. Martin (2024) Chapters 5 and 10 ("Comparing Models," "Inference Engines"). Apply variational Bayes and approximations in PyMC for efficient model fitting.</p>
    </div>
  </details>
  
  <h2>Part 2: Advanced Topics (Weeks 13-18)</h2>
  
  <!-- Module 07 -->
  <details class="module-unread">
    <summary class="module-summary">Module 1: Missing Data and Nonlinear Models (Weeks 13-14)</summary>
    <div id="mod7">
      <p>This module addresses handling incomplete datasets through imputation and extends to parametric nonlinear models, teaching how to incorporate nonlinearity while maintaining Bayesian principles for robust estimation.</p>
      <p><span id="mod7-prereq" class="module-item">Prerequisites</span>. Approximations and computation from Part 1 Module 6.</p>
      <p><span id="mod7-theory" class="module-item">Theory</span>. Gelman et al. (2014) Chapters 18-19 ("Models for Missing Data," "Nonlinear Models: Parametric"). Cover multiple imputation and nonlinear parameterizations.</p>
      <p><span id="mod7-apps" class="module-item">Applications</span>. Bryant and Zhang (2018) Chapters 18-19 ("Population Accounts in New Zealand," "Population in China") for missing data in demographic accounts and forecasts.</p>
      <p><span id="mod7-hands" class="module-item">Hands-on</span>. Martin (2024) Chapters 2 and 7 ("Programming Probabilistically," "Mixture Models"). Use PyMC for robust models handling missingness and mixtures.</p>
    </div>
  </details>
  
  <!-- Module 08 -->
  <details class="module-unread">
    <summary class="module-summary">Module 2: Basis Functions and Gaussian Processes (Weeks 15-16)</summary>
    <div id="mod8">
      <p>You'll explore flexible modeling with basis expansions (e.g., splines) and Gaussian processes for capturing smooth, non-linear relationships in data, ideal for time series or spatial patterns.</p>
      <p><span id="mod8-prereq" class="module-item">Prerequisites</span>. Nonlinear models and missing data from Module 1.</p>
      <p><span id="mod8-theory" class="module-item">Theory</span>. Gelman et al. (2014) Chapters 20-21 ("Basis Function Models," "Gaussian Process Models"). Focus on splines, kernels, and GP priors.</p>
      <p><span id="mod8-apps" class="module-item">Applications</span>. Bryant and Zhang (2018) Chapters 15-16 ("Migration in Iceland," "Fertility in Cambodia") for GPs in spatial migration or temporal fertility trends; add Gelman's GP examples in political trends if fitting.</p>
      <p><span id="mod8-hands" class="module-item">Hands-on</span>. Martin (2024) Chapter 8 ("Gaussian Processes"). Implement GP regression and classification in PyMC, such as for spatial data.</p>
    </div>
  </details>
  
  <!-- Module 09 -->
  <details class="module-unread">
    <summary class="module-summary">Module 3: Mixtures and Dirichlet Processes (Weeks 17-18)</summary>
    <div id="mod9">
      <p>This final module introduces nonparametric approaches like finite mixtures and Dirichlet processes for modeling heterogeneous data without fixed structures, enabling discovery of latent groups.</p>
      <p><span id="mod9-prereq" class="module-item">Prerequisites</span>. Gaussian processes and basis functions from Module 2.</p>
      <p><span id="mod9-theory" class="module-item">Theory</span>. Gelman et al. (2014) Chapters 22-23 ("Finite Mixture Models," "Dirichlet Process Models"). Emphasize clustering and infinite mixtures.</p>
      <p><span id="mod9-apps" class="module-item">Applications</span>. Bryant and Zhang (2018) Chapters 11-12 ("Mortality in Sweden," "Life Expectancy in Portugal") for mixture-based demographic rates and expectancies.</p>
      <p><span id="mod9-hands" class="module-item">Hands-on</span>. Martin (2024) Chapters 7 and 9 ("Mixture Models," "Bayesian Additive Regression Trees"). Code finite/non-finite mixtures and related trees in PyMC/Bambi.</p>
    </div>
  </details>
</body>
</html>